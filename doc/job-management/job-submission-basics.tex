% Job Submission Basics
% -------------------------------------------------------------
\subsection{Job Submission Basics}
\label{sect:job-submission-basics}

Preparing your job for submission is fairly straightforward.
Start by basing your job script on one of the examples available in the \texttt{src/}
directory of our \href{https://github.com/NAG-DevOps/speed-hpc}{GitHub repository}.
You can clone the repository to get the examples to start with via the command line:

\begin{verbatim}
    git clone --depth=1 https://github.com/NAG-DevOps/speed-hpc.git
    cd speed-hpc/src
\end{verbatim}

\noindent The job script is a shell script that contains directives, module loads, and user scripting.
To quickly run some sample jobs, use the following commands:
\begin{verbatim}
    sbatch -p ps -t 10 env.sh
    sbatch -p ps -t 10 bash.sh
    sbatch -p ps -t 10 manual.sh
    sbatch -p pg -t 10 lambdal-singularity.sh
\end{verbatim}

% Directives
% -------------------
\subsubsection{Directives}
\label{sect:directives}

Directives are comments included at the beginning of a job script that set the shell
and the options for the job scheduler.

The shebang directive is always the first line of a script. 
In your job script, this directive sets which shell your script's commands will run in. 
On ``Speed'', we recommend that your script use a shell from the \texttt{/encs/bin} directory.

To specify the shell for your script, begin with the appropriate shebang line:\\
use use \verb|#!/encs/bin/tcsh| for the tcsh shell, or \verb|#!/encs/bin/bash| for bash.

Directives that start with \verb|#SBATCH| set the options for the cluster's SLURM job scheduler.
The following provides an example of some essential directives:

\small
\begin{verbatim}
    #SBATCH --job-name=<jobname>        ## or -J. Give the job a name
    #SBATCH --mail-type=<type>          ## set type of email notifications
    #SBATCH --chdir=<directory>         ## or -D, set working directory for the job
    #SBATCH --nodes=1                   ## or -N, node count required for the job
    #SBATCH --ntasks=1                  ## or -n, number of tasks to be launched
    #SBATCH --cpus-per-task=<corecount> ## or -c, core count requested, e.g. 8 cores
    #SBATCH --mem=<memory>              ## assign memory for this job,
                                        ## e.g., 32G memory per node
\end{verbatim}
\normalsize

\noindent Replace the following to adjust the job script for your project(s)
\begin{itemize}
    \item \verb+<jobname>+ with a job name for the job. This name will be displayed in the job queue.
    \item \verb+<directory>+ with the fullpath to your job's working directory, e.g., where your code,
    source files and where the standard output files will be written to.
    By default, \verb+--chdir+ sets the current directory as the job's working directory.
    \item \verb+<type>+ with the type of e-mail notifications you wish to receive.
    Valid options are: NONE, BEGIN, END, FAIL, REQUEUE, ALL.
    \item \verb+<corecount>+ with the degree of multithreaded parallelism (i.e., cores) allocated to your job. Up to 32 by default.
    \item \verb+<memory>+ with the amount of memory, in GB, that you want to be allocated per node. Up to 500 depending on the node.\\
    \textbf{Note}: All jobs MUST set a value for the \option{--mem} option.
\end{itemize}

\noindent Example with short option equivalents:
\small
\begin{verbatim}
    #SBATCH -J myjob              ## Job's name set to 'myjob'
    #SBATCH --mail-type=ALL       ## Receive all email type notifications
    #SBATCH -D ./                 ## Use current directory as working directory
    #SBATCH -N 1                  ## Node count required for the job
    #SBATCH -n 1                  ## Number of tasks to be launched
    #SBATCH -c 8                  ## Request 8 cores
    #SBATCH --mem=32G             ## Allocate 32G memory per node
\end{verbatim}
\normalsize

\noindent \textbf{Tip:} If you are unsure about memory footprints, err on assigning a generous
memory space to your job, so that it does not get prematurely terminated.
You can refine \option{--mem} values for future jobs by monitoring the size of a job's active
memory space on \texttt{speed-submit} with:

\begin{verbatim}
    sacct -j <jobID>
    sstat -j <jobID>
\end{verbatim}

\noindent This can be customized to show specific columns:

\begin{verbatim}
    sacct -o jobid,maxvmsize,ntasks%7,tresusageouttot%25 -j <jobID>
    sstat -o jobid,maxvmsize,ntasks%7,tresusageouttot%25 -j <jobID>
\end{verbatim}

\noindent Memory-footprint efficiency values \tool{seff} are also provided for completed jobs
in the final email notification as ``maxvmsize''.

\emph{Jobs that request a low-memory footprint are more likely to load on a busy cluster.}

\noindent Other essential options are \option{--time}, or \option{-t}, and \option{--account}, or \option{-A}.
\begin{itemize}
    \item \option{--time=<time>} -- is the estimate of wall clock time required for your job to run.
    As previously mentioned, the maximum is 7 days for batch and 24 hours for interactive jobs.
    Jobs with a smaller \texttt{time} value will have a higher priority and may result in your job being scheduled sooner.
    \item \option{--account=<name>} -- specifies which Account, aka project or association,
    that the Speed resources your job uses should be attributed to. When moving from
    GE to SLURM users most users were assigned to Speed's two default accounts
    \texttt{speed1} and \texttt{speed2}. However, users that belong to a particular research
    group or project are will have a default Account like the following
    \texttt{aits},
    \texttt{vidpro},
    \texttt{gipsy},
    \texttt{ai2},
    \texttt{mpackir},
    \texttt{cmos}, among others.
\end{itemize}

% Module Loads
% -------------------
\subsubsection{Working with Modules}
\label{sect:modules}

After setting the directives in your job script, the next section typically involves loading
the necessary software modules. The \tool{module} command is used to manage the user environment,
make sure to load all the modules your job depends on. You can check available modules with the
module avail command. Loading the correct modules ensures that your environment is properly
set up for execution.

\noindent To list for a particular program (\tool{matlab}, for example):
\small
\begin{verbatim}
    module avail
   module -t avail matlab  ## show the list for a particular program (e.g., matlab)
    module -t avail m       ## show the list for all programs starting with `m'
\end{verbatim}
\normalsize

For example, insert the following in your script to load the \tool{matlab/R2023a} module:
\begin{verbatim}
    module load matlab/R2023a/default
\end{verbatim}

\textbf{Note:} you can remove a module from active use by replacing \option{load} by \option{unload}.

To list loaded modules:
\begin{verbatim}
    module list
\end{verbatim}

To purge all software in your working environment:
\begin{verbatim}
    module purge
\end{verbatim}

% User Scripting
% -------------------
\subsubsection{User Scripting}
\label{sect:scripting}

The final part of the job script involves the commands that will be executed by the job.
This section should include all necessary commands to set up and run the tasks
your script is designed to perform. You can use any Linux command in this section,
ranging from a simple executable call to a complex loop iterating through multiple commands.

\noindent \textbf{Best Practice}: prefix any compute-heavy step with \tool{srun}.
This ensures you gain proper insights on the execution of your job.

\noindent Each software program may have its own execution framework, as it's the script's author (e.g., you)
responsibility to review the software's documentation to understand its requirements.
Your script should be written to clearly specify the location of input and output files and the degree of parallelism needed.

\noindent Jobs that involve multiple interactions with data input and output files, should make use of \api{TMPDIR},
a scheduler-provided workspace nearly 1~TB in size.
\api{TMPDIR} is created on the local disk of the compute node at the start of a job, offering faster I/O operations
compared to shared storage (provided over NFS).

\noindent An sample job script using \api{TMPDIR} is \texttt{tmpdir.sh}.

\small
\begin{figure}[htpb]
	\lstinputlisting[language=csh,frame=single,basicstyle=\ttfamily\scriptsize]{tmpdir.sh}
	\caption{Source code for \file{tmpdir.sh}}
	\label{fig:tmpdir.sh}
\end{figure}
\normalsize

\noindent The job script begins by switching to the temporary working directory defined by \api{TMPDIR}.
It then creates a new subdirectory called \texttt{input} and copies data from the
\texttt{\$SLURM\_SUBMIT\_DIR/references/} which represents the job submission directory into this created folder.
A second directory, \texttt{results}, is also created to store the output. The job then executes the program using \texttt{srun}, 
reading from \texttt{\$TMPDIR/input/} and writing results to \texttt{\$TMPDIR/results/}. 
Once the computation is complete, the output files are copied back to the \texttt{processed} directory in the original submission folder.

\noindent \textbf{Note:} \api{TMPDIR} only exists for the duration of the job, so it is crucial to copy relevant results from it at the end of the job.
