In these instructions, anything bracketed like so, <>, indicates a
label/value to be replaced (the entire bracketed term needs replacement).

If you are connecting from home, and have a Mac or Linux system, in a
terminal, this will connect you:

ssh -o ProxyCommand="ssh <ENCSusername>@login.encs.concordia.ca nc speed
22" <ENCSusername>@speed.encs.concordia.ca

Windows users can connect via PuTTY (or MobaXterm). All users are expected
to have a basic understanding of Linux and its commonly used commands.

As a new user, please execute the following the first time that you log
in, in your home directory, *exactly* as it is written (it is all one
line):

cp /home/n/nul-uge/.tcshrc . && cp /home/n/nul-uge/template.sh . && mkdir
/speed-scratch/$USER

That command prepares your environment (careful overwriting an existing
.tcshrc), provides you with a job template, and sets up your
cluster-specific storage. Note that you need to either log out and back
in, or execute a new shell, for the environment changes to be applied
(important). And if you use bash, or another shell, please contact 
rt-ex-hpc@encs.concordia.ca.

Preparing your job for submission is fairly straightforward. Job scripts
are broken into four main sections: make a copy of template.sh, and base
your job on that. The first section is the shell call, and that can be
left alone, unless you want to use a different shell. The next section
contains the options provided to the cluster scheduler, and template.sh
provides the essentials:

#$ -N <jobname>
#$ -cwd
#$ -m bea
#$ -pe smp <corecount>
#$ -l h_vmem=<memory>G

Replace, <jobname>, with the name that you want your cluster job to have;
'-cwd', makes the current working directory the "job working directory",
and your standard output file will appear here; '-m bea', provides e-mail
notifications (begin/end/abort); replace, <corecount>, with the degree of
(multithreaded) parallelism (i.e., cores) you attach to your job (up to
32), and this line needs to be deleted or commented out if not relevant;
replace, <memory>, with the value (in GB), that you want your job's memory
space to be (up to 500), and all jobs MUST have a memory-space assignment.
If you are unsure about memory footprints, err on assigning a generous
memory space to your job so that it does not get prematurely terminated
(the value given to h_vmem is a hard memory ceiling). You can refine
h_vmem values for future jobs by monitoring the size of a job's active
memory space on speed-submit with:

qstat -j <jobID> | grep maxvmem

Memory-footprint values are also provided for completed jobs in the final
e-mail notification (as, "Max vmem"), and via the accounting function on
speed-submit with:

qacct -j <jobID> | grep maxvmem

Jobs that request a low-memory footprint are more likely to load on a busy
cluster.

The third job-script section is for module loads. On speed-submit, to list
what is available:

module avail

To list for a particular program (matlab, for example):

module -t avail matlab

Which, of course, can be shortened to match all that start with a
particular letter:

module -t avail o

To load a module (here, again, using matlab):

module load matlab/R2020a/default

Use, 'unload', in place of, 'load', to remove a module from active use.

Finally, to list loaded modules:

module list

Typically only the 'module load' command will be used in your script.

Lastly, what you want to execute in your job goes into the fourth section
of the script. A common execution framework comprises the program call,
and options to the program specifying location of input and output files,
as well as a declaration of degree of parallelism (note the
cluster-specific environment variable, NSLOTS, which resolves to the value
provided to the scheduler option, '-pe smp'), though, obviously, every
program has its own unique execution framework.

Please note that jobs that expect to touch data-input and -output files
more than once should make use of TMPDIR, a scheduler-provided working
space (this is quite generous, close to 1 TB in size). TMPDIR is created
when a job starts, and exists on the local disk of the compute node
executing your job, so I/O operations are much faster than what would be
expected to/from the shared storage (which is provided over NFS). An
example job script using TMPDIR is found at
/home/n/nul-uge/templateTMPDIR.sh: the job is instructed to change to
$TMPDIR, to make the new directory "input", to copy data from
"$SGE_O_WORKDIR/references/" to "input/" ($SGE_O_WORKDIR represents the
current working directory), to make the new directory "results", to
execute the program (which takes input from "$TMPDIR/input/“ and writes
output to "$TMPDIR/results/“), and finally to copy the total end results
to an existing directory, "processed", that is located in the current
working directory. TMPDIR only exists for the duration of the job, though,
so it is very important to copy relevant results from it at job's end.

Once that your job script is ready, on speed-submit you can submit it
with:

qsub ./<myscript>.sh

You can check the status of your job(s) with:

qstat -f -u <ENCSusername>

And to see all jobs:

qstat -f -u "*"

If you need to delete a job, the syntax is:

qdel <jobID>

All jobs generate a standard output file, in the directory that the job
was submitted from, in the name format of, "<jobname>.o<jobID>". Important
information is often written to this file.


Interactive jobs can be scheduled, instead of batch (script) ones. 
Contact rt-ex-hpc@encs.concordia.ca for more information.

MPI-based jobs need SSH keys set up. 

There is a queue set aside specifically for GPU-based jobs.
Contact rt-ex-hpc@encs.concordia.ca for more information.

Job sessions are a maximum of one week in length (only 24 hours, though,
for interactive jobs).

Scripts can live in your NFS-provided home, but any substantial data need
to be in your cluster-specific directory (located at
/speed-scratch/<ENCSusername>/).

New users are restricted to a total of 32 cores: write to rt-ex-hpc@encs.concordia.ca
 if you need more (256 is the maximum possible, or, 8 jobs of 32 cores each).

The cluster is, "first come, first served", until it fills, and then job
position in the queue is based upon past usage. The scheduler does attempt
to fill gaps, though, so sometimes a single-core job of lower priority
will schedule before a multi-core job of higher priority, for example.

Contact rt-ex-hpc@encs.concordia.ca with your questions and for more information.
